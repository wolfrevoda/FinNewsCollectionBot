# ç¦ç”Ÿæ— é‡å¤©å°Š
from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os

# OpenAI API Key
openai_api_key = os.getenv("OPENAI_API_KEY")
# ä»ç¯å¢ƒå˜é‡è·å– Serveré…± SendKeys
server_chan_keys_env = os.getenv("SERVER_CHAN_KEYS")
if not server_chan_keys_env:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")
server_chan_keys = server_chan_keys_env.split(",")

openai_client = OpenAI(api_key=openai_api_key, base_url="https://api.deepseek.com/v1")

# RSSæºåœ°å€åˆ—è¡¨
rss_feeds = {
    "ğŸ’² åå°”è¡—è§é—»":{
        "åå°”è¡—è§é—»":"https://dedicated.wallstreetcn.com/rss.xml",      
    },
    "ğŸ’» 36æ°ª":{
        "36æ°ª":"https://36kr.com/feed",   
        },
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±":"https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ":"http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹":"http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘":"https://www.chinanews.com.cn/rss/finance.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ":"https://www.stats.gov.cn/sj/zxfb/rss.xml",
    },
      "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness",
        "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº":"https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
        "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
        "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
        "ETF Trends": "https://www.etftrends.com/feed/",
    },
    "ğŸŒ ä¸–ç•Œç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
        "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    },
}

# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()

# çˆ¬å–ç½‘é¡µæ­£æ–‡ (ç”¨äº AI åˆ†æï¼Œä½†ä¸å±•ç¤º)
def fetch_article_text(url):
    try:
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–‡ç« å†…å®¹: {url}")
        article = Article(url)
        article.download()
        article.parse()
        text = article.text[:1500]  # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è¶…å‡º API è¾“å…¥é™åˆ¶
        if not text:
            print(f"âš ï¸ æ–‡ç« å†…å®¹ä¸ºç©º: {url}")
        return text
    except Exception as e:
        print(f"âŒ æ–‡ç« çˆ¬å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
        return "ï¼ˆæœªèƒ½è·å–æ–‡ç« æ­£æ–‡ï¼‰"

# æ·»åŠ  User-Agent å¤´
def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    return feedparser.parse(url, request_headers=headers)


# è‡ªåŠ¨é‡è¯•è·å– RSS
def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
            time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None

# è·å–RSSå†…å®¹ï¼ˆçˆ¬å–æ­£æ–‡ä½†ä¸å±•ç¤ºï¼‰
def fetch_rss_articles(rss_feeds, max_articles=10):
    news_data = {}
    analysis_text = ""  # ç”¨äºAIåˆ†æçš„æ­£æ–‡å†…å®¹

    for category, sources in rss_feeds.items():
        category_content = ""
        for source, url in sources.items():
            print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")
            feed = fetch_feed_with_retry(url)
            if not feed:
                print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                continue
            print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

            articles = []  # æ¯ä¸ªsourceéƒ½éœ€è¦é‡æ–°åˆå§‹åŒ–åˆ—è¡¨
            for entry in feed.entries[:5]:
                title = entry.get('title', 'æ— æ ‡é¢˜')
                link = entry.get('link', '') or entry.get('guid', '')
                if not link:
                    print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                    continue

                # çˆ¬å–æ­£æ–‡ç”¨äºåˆ†æï¼ˆä¸å±•ç¤ºï¼‰
                article_text = fetch_article_text(link)
                analysis_text += f"ã€{title}ã€‘\n{article_text}\n\n"

                print(f"ğŸ”¹ {source} - {title} è·å–æˆåŠŸ")
                articles.append(f"- [{title}]({link})")

            if articles:
                category_content += f"### {source}\n" + "\n".join(articles) + "\n\n"

        news_data[category] = category_content

    return news_data, analysis_text

# AI ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼ˆåŸºäºçˆ¬å–çš„æ­£æ–‡ï¼‰
def summarize(text):
    completion = openai_client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "âœ…ã€ä¸“ä¸šè´¢ç»è¦ç‚¹æç‚¼ Prompt æ¨¡æ¿ã€‘

ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œã€é€»è¾‘ä¸¥è°¨çš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼ŒæœåŠ¡å¯¹è±¡ä¸ºåˆ¸å•†åˆ†æå¸ˆã€åŸºé‡‘ç»ç†ã€é‡‘èç ”ç©¶å‘˜ã€å®è§‚ç­–ç•¥å¸ˆç­‰ä¸“ä¸šäººå£«ã€‚

è¯·åŸºäºä»¥ä¸‹è´¢ç»æ–°é—»åŸæ–‡å†…å®¹ï¼Œå®Œæˆé«˜è´¨é‡çš„å†…å®¹ç†è§£ä¸ç»“æ„åŒ–æ€»ç»“ï¼Œå½¢æˆä¸€ä»½ä¸“ä¸šã€ç²¾å‡†ã€æ¸…æ™°çš„è´¢ç»è¦ç‚¹æ‘˜è¦ï¼Œç”¨äºæ”¯æŒæœºæ„æŠ•èµ„è€…çš„æ—¥å¸¸ç ”åˆ¤å·¥ä½œã€‚

ğŸ¯ã€è¾“å‡ºè¦æ±‚ã€‘
	1.	å…¨æ–‡æ§åˆ¶åœ¨ 2000 å­—ä»¥å†…ï¼Œå†…å®¹ç²¾ç‚¼ã€é€»è¾‘æ¸…æ™°ï¼›
	2.	ä»å®è§‚æ”¿ç­–ã€é‡‘èå¸‚åœºã€è¡Œä¸šåŠ¨æ€ã€å…¬å¸äº‹ä»¶ã€é£é™©æç¤ºç­‰è§’åº¦è¿›è¡Œåˆ†ç±»æ€»ç»“ï¼›
	3.	æ¯ä¸€éƒ¨åˆ†è¦çªå‡ºæ•°æ®æ”¯æŒã€è¶‹åŠ¿ç ”åˆ¤ã€å¯èƒ½çš„å¸‚åœºå½±å“ï¼›
	4.	æ˜ç¡®æŒ‡å‡ºæ–°é—»èƒŒåçš„æ ¸å¿ƒå˜é‡æˆ–æ”¿ç­–æ„å›¾ï¼Œå¹¶æå‡ºæŠ•èµ„è§†è§’ä¸‹çš„å‚è€ƒæ„ä¹‰ï¼›
	5.	è¯­æ°”ä¸“ä¸šã€ä¸¥è°¨ã€æ— æƒ…ç»ªåŒ–è¡¨è¾¾ï¼Œé€‚é…ä¸“ä¸šæœºæ„æŠ•ç ”é˜…è¯»ä¹ æƒ¯ï¼›
	6.	ç¦æ­¢å¥—è¯ï¼Œä¸é‡å¤æ–°é—»åŸæ–‡ï¼Œå¯ç”¨æ¡åˆ—å¼å¢å¼ºç»“æ„æ€§ï¼›
	7.	å¦‚æ¶‰åŠæ•°æ®å’Œé¢„æµ‹ï¼Œè¯·æ ‡æ³¨æ¥æºæˆ–æŒ‡å‡ºä¸»å¼ æœºæ„ï¼ˆå¦‚é«˜ç››ã€èŠ±æ——ç­‰ï¼‰ï¼›
	8.	è‹¥åŸæ–‡è¾ƒå¤šå†…å®¹æ— å…³è´¢ç»å¸‚åœºï¼Œå¯é…Œæƒ…ç•¥å»ï¼Œåªä¿ç•™å…³é”®å½±å“è¦ç´ ã€‚"},
            {"role": "user", "content": text}
        ]
    )
    return completion.choices[0].message.content.strip()

# å‘é€å¾®ä¿¡æ¨é€
def send_to_wechat(title, content):
    for key in server_chan_keys:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        response = requests.post(url, data=data, timeout=10)
        if response.ok:
            print(f"âœ… æ¨é€æˆåŠŸ: {key}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")


if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")

    # æ¯ä¸ªç½‘ç«™è·å–æœ€å¤š 5 ç¯‡æ–‡ç« 
    articles_data, analysis_text = fetch_rss_articles(rss_feeds, max_articles=5)
    
    # AIç”Ÿæˆæ‘˜è¦
    summary = summarize(analysis_text)

    # ç”Ÿæˆä»…å±•ç¤ºæ ‡é¢˜å’Œé“¾æ¥çš„æœ€ç»ˆæ¶ˆæ¯
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content in articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"

    # æ¨é€åˆ°å¤šä¸ªserveré…±key
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
