# ç¦ç”Ÿæ— é‡å¤©å°Š
from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os

# OpenAI API Key
# openai_client = OpenAI(api_key="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")
openai_api_key = os.getenv("OPENAI_API_KEY")
server_chan_key = os.getenv("SERVER_CHAN_KEY")
openai_client = OpenAI(api_key=openai_api_key)


# Serveré…±SendKey
# srz çš„ SCT272699TfiTnNWUMAMHjvajyebNd6B8N
# SERVER_CHAN_KEY = "SCT272745TdQMzTMudpFDrYGFr4XOrBBgL"

# RSSæºåœ°å€åˆ—è¡¨
rss_feeds = {
    "ğŸ’² åå°”è¡—è§é—»":{
        "åå°”è¡—è§é—»":"https://dedicated.wallstreetcn.com/rss.xml",      
    },
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "ä¸œæ–¹è´¢å¯Œ":"http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹":"http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘":"https://www.chinanews.com.cn/rss/finance.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ":"https://www.stats.gov.cn/sj/zxfb/rss.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æ•°æ®è§£è¯»":"https://www.stats.gov.cn/sj/sjjd/rss.xml",
    },
      "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
        "CNN Money" :"http://rss.cnn.com/rss/money_topstories.rss",
        "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
        "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
        "ETF Trends": "https://www.etftrends.com/feed/",
    },
    "ğŸŒ ä¸–ç•Œç»æµ": {
        "åå°”è¡—æ—¥æŠ¥":"https://cn.wsj.com/zh-hans/rss",
        "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    },
}

# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()


# çˆ¬å–ç½‘é¡µæ­£æ–‡
def fetch_article_text(url):
    try:
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–‡ç« å†…å®¹: {url}")
        article = Article(url)
        article.download()
        article.parse()
        text = article.text[:1500]
        if not text:
            print(f"âš ï¸ æ–‡ç« å†…å®¹ä¸ºç©º: {url}")
        return text
    except Exception as e:
        print(f"âŒ æ–‡ç« çˆ¬å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
        return "ï¼ˆæœªèƒ½è·å–æ–‡ç« æ­£æ–‡ï¼‰"

# æ·»åŠ  User-Agent å¤´
def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    return feedparser.parse(url, request_headers=headers)


# è‡ªåŠ¨é‡è¯•è·å– RSS
def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
            time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None

# è·å–RSSå†…å®¹å¹¶çˆ¬å–æ–‡ç« æ­£æ–‡
def fetch_rss_articles(rss_feeds, max_articles=10):
    news_data = {}
    today = today_date()

    for category, sources in rss_feeds.items():
        category_content = ""
        for source, url in sources.items():
            print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")
            feed = fetch_feed_with_retry(url)
            if not feed:
                print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                continue
            print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

            articles = []
            for entry in feed.entries[:max_articles]:
                title = entry.get('title', 'æ— æ ‡é¢˜')
                link = entry.get('link', '') or entry.get('guid', '')
                if not link:
                    print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                    continue

                article_text = fetch_article_text(link)
                print(f"ğŸ”¹ {source} - {title} è·å–æˆåŠŸ")

                articles.append(f"- {title}\n  {article_text}\n  [æŸ¥çœ‹åŸæ–‡]({link})\n")
            
            if articles:
                category_content += f"### {source}\n" + "\n".join(articles) + "\n\n"
        
        news_data[category] = category_content
    
    return news_data

# AIç”Ÿæˆå†…å®¹æ‘˜è¦
def summarize(text):
    completion = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ï¼Œæç‚¼å‡ºæœ€æ ¸å¿ƒçš„è¦ç‚¹ï¼Œæä¾›ä¸€ä»½1000å­—ä»¥å†…çš„ä¸­æ–‡æ¸…æ™°æ‘˜è¦ã€‚è¯·ç¡®ä¿æ€»ç»“ç²¾å‡†ã€é€»è¾‘æ¸…æ™°ï¼Œå¹¶çªå‡ºè´¢ç»é¢†åŸŸçš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®æ•°æ®ï¼Œé¿å…å†—ä½™ä¿¡æ¯ã€‚"},
            {"role": "user", "content": text}
        ]
    )
    return completion.choices[0].message.content.strip()

# å¾®ä¿¡æ¨é€
# å‘é€å¾®ä¿¡æ¨é€
def send_to_wechat(title, content):
    requests.post(f"https://sctapi.ftqq.com/{server_chan_key}.send", data={
        "title": title,
        "desp": content
    })

# ä¸»ç¨‹åº
if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")
    #æ¯ä¸ªç½‘ç«™è·å–æœ€å¤š5ç¯‡æ–‡ç« 
    articles = fetch_rss_articles(rss_feeds, max_articles = 5 ) 

    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\n"
    for category, content in articles.items():
        if content.strip():
            summary = summarize(content)
            final_summary += f"## {category}\nâœï¸ **ä»Šæ—¥æ€»ç»“ï¼š** {summary}\n\n\n---\n\n{content}\n\n"
    
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)