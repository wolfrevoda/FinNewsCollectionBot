# ç¦ç”Ÿæ— é‡å¤©å°Š
from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
import argparse

class ArticleFetcher:
    """Class for fetching articles from RSS feeds"""
    
    RSS_FEEDS = {
        "ğŸ’² åå°”è¡—è§é—»":{
            "åå°”è¡—è§é—»":"https://dedicated.wallstreetcn.com/rss.xml",      
        },
        "ğŸ’» 36æ°ª":{
            "36æ°ª":"https://36kr.com/feed",   
        },
        "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
            "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±":"https://www.hket.com/rss/china",
            "ä¸œæ–¹è´¢å¯Œ":"http://rss.eastmoney.com/rss_partener.xml",
            "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹":"http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
            "ä¸­æ–°ç½‘":"https://www.chinanews.com.cn/rss/finance.xml",
            "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ":"https://www.stats.gov.cn/sj/zxfb/rss.xml",
        },
        "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
            "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness",
            "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº":"https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
            "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
            "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
            "ETF Trends": "https://www.etftrends.com/feed/",
        },
        "ğŸŒ ä¸–ç•Œç»æµ": {
            "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
            "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
        },
    }

    @staticmethod
    def today_date():
        """Get current Beijing date"""
        return datetime.now(pytz.timezone("Asia/Shanghai")).date()

    @staticmethod
    def fetch_article_text(url):
        """Fetch article content from URL"""
        try:
            print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–‡ç« å†…å®¹: {url}")
            article = Article(url)
            article.download()
            article.parse()
            text = article.text[:1500]  # Limit length to prevent API input overflow
            if not text:
                print(f"âš ï¸ æ–‡ç« å†…å®¹ä¸ºç©º: {url}")
            return text
        except Exception as e:
            print(f"âŒ æ–‡ç« çˆ¬å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
            return "ï¼ˆæœªèƒ½è·å–æ–‡ç« æ­£æ–‡ï¼‰"

    @staticmethod
    def fetch_feed_with_headers(url):
        """Fetch RSS feed with headers"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        return feedparser.parse(url, request_headers=headers)

    @staticmethod
    def fetch_feed_with_retry(url, retries=3, delay=5):
        """Fetch RSS feed with retry mechanism"""
        for i in range(retries):
            try:
                feed = ArticleFetcher.fetch_feed_with_headers(url)
                if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                    return feed
            except Exception as e:
                print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
                time.sleep(delay)
        print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
        return None

    def fetch_rss_articles(self, max_articles=10):
        """Fetch articles from all RSS feeds"""
        news_data = {}
        analysis_text = ""  # Text for AI analysis

        for category, sources in self.RSS_FEEDS.items():
            category_content = ""
            for source, url in sources.items():
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")
                feed = self.fetch_feed_with_retry(url)
                if not feed:
                    print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                    continue
                print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

                articles = []
                for entry in feed.entries[:5]:
                    title = entry.get('title', 'æ— æ ‡é¢˜')
                    link = entry.get('link', '') or entry.get('guid', '')
                    if not link:
                        print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                        continue

                    article_text = self.fetch_article_text(link)
                    analysis_text += f"ã€{title}ã€‘\n{article_text}\n\n"

                    print(f"ğŸ”¹ {source} - {title} è·å–æˆåŠŸ")
                    articles.append(f"- [{title}]({link})")

                if articles:
                    category_content += f"### {source}\n" + "\n".join(articles) + "\n\n"

            news_data[category] = category_content

        return news_data, analysis_text


class ArticleSummarizer:
    """Class for summarizing articles using DeepSeek API"""
    
    def __init__(self, api_key):
        """Initialize with API key"""
        self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    def summarize(self, text):
        """Generate summary using DeepSeek API"""
        completion = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œã€é€»è¾‘ä¸¥è°¨çš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼ŒæœåŠ¡å¯¹è±¡ä¸ºåˆ¸å•†åˆ†æå¸ˆã€åŸºé‡‘ç»ç†ã€é‡‘èç ”ç©¶å‘˜ã€å®è§‚ç­–ç•¥å¸ˆç­‰ä¸“ä¸šäººå£«ã€‚è¯·åŸºäºä»¥ä¸‹è´¢ç»æ–°é—»åŸæ–‡å†…å®¹ï¼Œå®Œæˆé«˜è´¨é‡çš„å†…å®¹ç†è§£ä¸ç»“æ„åŒ–æ€»ç»“ï¼Œå½¢æˆä¸€ä»½ä¸“ä¸šã€ç²¾å‡†ã€æ¸…æ™°çš„è´¢ç»è¦ç‚¹æ‘˜è¦ï¼Œç”¨äºæ”¯æŒæœºæ„æŠ•èµ„è€…çš„æ—¥å¸¸ç ”åˆ¤å·¥ä½œã€‚ã€è¾“å‡ºè¦æ±‚ã€‘1.å…¨æ–‡æ§åˆ¶åœ¨ 2000 å­—ä»¥å†…ï¼Œå†…å®¹ç²¾ç‚¼ã€é€»è¾‘æ¸…æ™°ï¼›2.ä»å®è§‚æ”¿ç­–ã€é‡‘èå¸‚åœºã€è¡Œä¸šåŠ¨æ€ã€å…¬å¸äº‹ä»¶ã€é£é™©æç¤ºç­‰è§’åº¦è¿›è¡Œåˆ†ç±»æ€»ç»“ï¼›3.æ¯ä¸€éƒ¨åˆ†è¦çªå‡ºæ•°æ®æ”¯æŒã€è¶‹åŠ¿ç ”åˆ¤ã€å¯èƒ½çš„å¸‚åœºå½±å“ï¼›4.æ˜ç¡®æŒ‡å‡ºæ–°é—»èƒŒåçš„æ ¸å¿ƒå˜é‡æˆ–æ”¿ç­–æ„å›¾ï¼Œå¹¶æå‡ºæŠ•èµ„è§†è§’ä¸‹çš„å‚è€ƒæ„ä¹‰ï¼›5.è¯­æ°”ä¸“ä¸šã€ä¸¥è°¨ã€æ— æƒ…ç»ªåŒ–è¡¨è¾¾ï¼Œé€‚é…ä¸“ä¸šæœºæ„æŠ•ç ”é˜…è¯»ä¹ æƒ¯ï¼›6.ç¦æ­¢å¥—è¯ï¼Œä¸é‡å¤æ–°é—»åŸæ–‡ï¼Œå¯ç”¨æ¡åˆ—å¼å¢å¼ºç»“æ„æ€§ï¼›7.å¦‚æ¶‰åŠæ•°æ®å’Œé¢„æµ‹ï¼Œè¯·æ ‡æ³¨æ¥æºæˆ–æŒ‡å‡ºä¸»å¼ æœºæ„ï¼ˆå¦‚é«˜ç››ã€èŠ±æ——ç­‰ï¼‰ï¼›8.è‹¥åŸæ–‡è¾ƒå¤šå†…å®¹æ— å…³è´¢ç»å¸‚åœºï¼Œå¯é…Œæƒ…ç•¥å»ï¼Œåªä¿ç•™å…³é”®å½±å“è¦ç´ ã€‚"},
                {"role": "user", "content": text}
            ]
        )
        return completion.choices[0].message.content.strip()


class ResultPublisher:
    """Class for publishing results to WeChat"""
    
    def __init__(self, server_chan_keys):
        """Initialize with Serveré…± keys"""
        self.server_chan_keys = server_chan_keys.split(",") if isinstance(server_chan_keys, str) else server_chan_keys

    def send_to_wechat(self, title, content):
        """Send message to WeChat via Serveré…±"""
        for key in self.server_chan_keys:
            url = f"https://sctapi.ftqq.com/{key}.send"
            data = {"title": title, "desp": content}
            response = requests.post(url, data=data, timeout=10)
            if response.ok:
                print(f"âœ… æ¨é€æˆåŠŸ: {key}")
            else:
                print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")

    def generate_summary(self, news_data, summary, today_str):
        """Generate formatted summary message"""
        final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
        for category, content in news_data.items():
            if content.strip():
                final_summary += f"## {category}\n{content}\n\n"
        return final_summary


if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Finance News Collection Bot')
    parser.add_argument('--openai_key', required=True, help='OpenAI API key')
    parser.add_argument('--server_chan_keys', required=True, help='Serveré…± SendKeys, comma separated')
    args = parser.parse_args()

    # Initialize components
    fetcher = ArticleFetcher()
    summarizer = ArticleSummarizer(args.openai_key)
    publisher = ResultPublisher(args.server_chan_keys)

    # Get current date
    today_str = fetcher.today_date().strftime("%Y-%m-%d")
    
    # Fetch articles
    articles_data, analysis_text = fetcher.fetch_rss_articles(max_articles=5)
    
    # Generate summary
    summary = summarizer.summarize(analysis_text)

    # Publish results
    final_summary = publisher.generate_summary(articles_data, summary, today_str)
    publisher.send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
